{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17febfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6c1e282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>11.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2900 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
       "0                  4.0         No                      4.0            6.0   \n",
       "1                  9.0        Yes                      0.0            0.0   \n",
       "2                  9.0        Yes                      1.0            2.0   \n",
       "3                  0.0         No                      6.0            7.0   \n",
       "4                  3.0         No                      9.0            4.0   \n",
       "...                ...        ...                      ...            ...   \n",
       "2895               3.0         No                      7.0            6.0   \n",
       "2896               3.0         No                      8.0            3.0   \n",
       "2897               4.0        Yes                      1.0            1.0   \n",
       "2898              11.0        Yes                      1.0            3.0   \n",
       "2899               3.0         No                      6.0            6.0   \n",
       "\n",
       "     Drained_after_socializing  Friends_circle_size  Post_frequency  \\\n",
       "0                           No                 13.0             5.0   \n",
       "1                          Yes                  0.0             3.0   \n",
       "2                          Yes                  5.0             2.0   \n",
       "3                           No                 14.0             8.0   \n",
       "4                           No                  8.0             5.0   \n",
       "...                        ...                  ...             ...   \n",
       "2895                        No                  6.0             6.0   \n",
       "2896                        No                 14.0             9.0   \n",
       "2897                       Yes                  4.0             0.0   \n",
       "2898                       Yes                  2.0             0.0   \n",
       "2899                        No                  6.0             9.0   \n",
       "\n",
       "     Personality  \n",
       "0      Extrovert  \n",
       "1      Introvert  \n",
       "2      Introvert  \n",
       "3      Extrovert  \n",
       "4      Extrovert  \n",
       "...          ...  \n",
       "2895   Extrovert  \n",
       "2896   Extrovert  \n",
       "2897   Introvert  \n",
       "2898   Introvert  \n",
       "2899   Extrovert  \n",
       "\n",
       "[2900 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('personality_datasert.csv')\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d316cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "missing_value = df_raw.isnull().sum()\n",
    "missing_value = missing_value[missing_value > 0]\n",
    "missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55529eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['Stage_fear'] = df_raw['Stage_fear'].map({'Yes': 1, 'No': 0})\n",
    "df_raw['Drained_after_socializing'] = df_raw['Drained_after_socializing'].map({'Yes': 1, 'No': 0})\n",
    "df_raw['Personality'] = df_raw['Personality'].map({'Introvert': 0, 'Extrovert':1})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ded694f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time_spent_Alone  Stage_fear  Social_event_attendance  Going_outside  \\\n",
       "0               4.0           0                      4.0            6.0   \n",
       "1               9.0           1                      0.0            0.0   \n",
       "\n",
       "   Drained_after_socializing  Friends_circle_size  Post_frequency  Personality  \n",
       "0                          0                 13.0             5.0            1  \n",
       "1                          1                  0.0             3.0            0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63d8095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_kfold_pipeline(\n",
    "        df_raw: pd.DataFrame,\n",
    "        ft_cols_qualitative: list[str],\n",
    "        ft_cols_qualitative_to_one_hot: list[str],\n",
    "        ft_cols_quantitative: list[str],\n",
    "        target_col: str,\n",
    "        scaler,\n",
    "        model,\n",
    "        n_splits: int = 5,\n",
    "        random_state: int = 42,\n",
    "        shuffle: bool = True,\n",
    "        scoring=None\n",
    "    ):\n",
    "    \n",
    "    binary_flag_cols = list(set(ft_cols_qualitative) - set(ft_cols_qualitative_to_one_hot))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\",    scaler,                                   ft_cols_quantitative),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"),   ft_cols_qualitative_to_one_hot),\n",
    "            (\"binary\", \"passthrough\",                            binary_flag_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    X = df_raw.drop(columns=[target_col])\n",
    "    y = df_raw[target_col]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "    results = cross_validate(pipe, X, y, cv=cv, n_jobs=-1,\n",
    "                             return_train_score=True, scoring=scoring)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97361522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def run_exp(ft_cols_qualitative, ft_cols_qualitative_to_one_hot, ft_cols_quantitative, target_col):\n",
    "    list_max_iter = [10, 20, 30, 40, 50, 100, 200, 300, 500]\n",
    "    list_cs = [1,2,3,4,5,8,10]\n",
    "    best_parameters = {\n",
    "        \"max_iter\": None,\n",
    "        \"cs\": None,\n",
    "        \"test_acc\": -math.inf,\n",
    "        \"cv_scores\": None\n",
    "    }\n",
    "\n",
    "    for max_iter, cs in tqdm(itertools.product(list_max_iter, list_cs), \n",
    "                            total=len(list_max_iter) * len(list_cs), \n",
    "                            desc=\"Running Logistic Regression CV\"):\n",
    "        cv_run = run_kfold_pipeline(\n",
    "            df_raw=df_raw,\n",
    "            ft_cols_qualitative=ft_cols_qualitative,\n",
    "            ft_cols_qualitative_to_one_hot=ft_cols_qualitative_to_one_hot,\n",
    "            ft_cols_quantitative=ft_cols_quantitative,\n",
    "            target_col=target_col,\n",
    "            scaler=StandardScaler(),\n",
    "            model=LogisticRegressionCV(\n",
    "                max_iter=max_iter,\n",
    "                Cs=cs,\n",
    "            ),\n",
    "            scoring = {\n",
    "                \"acc\": \"accuracy\",\n",
    "                \"recall\": \"recall\",\n",
    "                \"precision\": \"precision\",\n",
    "                \"f1\": \"f1\",\n",
    "                \"roc_auc\": \"roc_auc\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        mean_test_acc = cv_run[\"test_acc\"].mean()\n",
    "        \n",
    "        if mean_test_acc > best_parameters[\"test_acc\"]:\n",
    "            best_parameters[\"max_iter\"] = max_iter\n",
    "            best_parameters[\"cs\"] = cs\n",
    "            best_parameters[\"test_acc\"] = mean_test_acc\n",
    "            best_parameters[\"cv_scores\"] = cv_run\n",
    "\n",
    "    print(f\"Best parameters LogClass: {best_parameters}\")\n",
    "    cv_scores_logistic = best_parameters[\"cv_scores\"]\n",
    "\n",
    "    print(pd.DataFrame(cv_scores_logistic).describe(percentiles=[]).loc[[\"mean\", \"std\", \"min\", \"max\"]])\n",
    "\n",
    "    list_max_depth = [2, 3, 5, 10]\n",
    "    list_n_estimators = [50, 100, 150, 200]\n",
    "    list_min_samples_leaf = [3, 5, 10, 20]\n",
    "\n",
    "    best_parameters = {\n",
    "        \"max_depth\": None,\n",
    "        \"n_estimators\": None,\n",
    "        \"min_samples_leaf\": None,\n",
    "        \"test_acc\": -math.inf,\n",
    "        \"cv_scores\": None\n",
    "    }\n",
    "\n",
    "    for max_depth, n_estimators, min_samples_leaf in tqdm(\n",
    "        itertools.product(list_max_depth, list_n_estimators, list_min_samples_leaf),\n",
    "        total=len(list_max_depth) * len(list_n_estimators) * len(list_min_samples_leaf)\n",
    "    ):    \n",
    "        cv_run = run_kfold_pipeline(\n",
    "            df_raw=df_raw,\n",
    "            ft_cols_qualitative=ft_cols_qualitative,\n",
    "            ft_cols_qualitative_to_one_hot=ft_cols_qualitative_to_one_hot,\n",
    "            ft_cols_quantitative=ft_cols_quantitative,\n",
    "            target_col=target_col,\n",
    "            scaler=StandardScaler(),\n",
    "            model=RandomForestClassifier(\n",
    "                max_depth=max_depth,\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "            ),\n",
    "            scoring = {\n",
    "                \"acc\": \"accuracy\",\n",
    "                \"recall\": \"recall\",\n",
    "                \"precision\": \"precision\",\n",
    "                \"f1\": \"f1\",\n",
    "                \"roc_auc\": \"roc_auc\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        mean_test_acc = cv_run['test_acc'].mean()\n",
    "        if mean_test_acc > best_parameters[\"test_acc\"]:\n",
    "            best_parameters[\"max_depth\"] = max_depth\n",
    "            best_parameters[\"n_estimators\"] = n_estimators\n",
    "            best_parameters[\"min_samples_leaf\"] = min_samples_leaf\n",
    "            best_parameters[\"test_acc\"] = mean_test_acc\n",
    "            best_parameters[\"cv_scores\"] = cv_run\n",
    "\n",
    "    print(f\"Best parameters: {best_parameters}\")\n",
    "\n",
    "    cv_scores_rf = best_parameters[\"cv_scores\"]\n",
    "\n",
    "    pd.DataFrame(cv_scores_rf).describe(percentiles=[]).loc[[\"mean\", \"std\", \"min\", \"max\"]]\n",
    "\n",
    "    list_max_depth = [ 6, 7, 8, 9]\n",
    "    list_lambda = [0.5, 1, 2]\n",
    "    list_learning_rate = [1, 0.8, 0.5, 0.25]\n",
    "    list_n_estimators = [50, 100, 150, 200]\n",
    "    list_alpha = [0.5, 1, 2]\n",
    "\n",
    "    best_parameters = {\n",
    "        \"max_depth\": None,\n",
    "        \"n_estimators\": None,\n",
    "        \"reg_lambda\": None,\n",
    "        \"reg_aplha\": None,\n",
    "        \"learning_rate\": None,\n",
    "        \"test_acc\": -math.inf,\n",
    "        \"cv_scores\": None\n",
    "    }\n",
    "\n",
    "    for max_depth, n_estimators, reg_lambda, lr, alpha in tqdm(\n",
    "        itertools.product(list_max_depth, list_n_estimators, list_lambda, list_learning_rate, list_alpha),\n",
    "        total=len(list_max_depth) * len(list_n_estimators) * len(list_lambda) * len(list_learning_rate) * len(list_alpha)\n",
    "    ):\n",
    "        cv_run = run_kfold_pipeline(\n",
    "            df_raw=df_raw,\n",
    "            ft_cols_qualitative=ft_cols_qualitative,\n",
    "            ft_cols_qualitative_to_one_hot=ft_cols_qualitative_to_one_hot,\n",
    "            ft_cols_quantitative=ft_cols_quantitative,\n",
    "            target_col=target_col,\n",
    "            scaler=StandardScaler(),\n",
    "            model=XGBClassifier(\n",
    "                max_depth=max_depth,\n",
    "                n_estimators=n_estimators,\n",
    "                reg_lambda=reg_lambda,\n",
    "                learning_rate=lr,\n",
    "                reg_alpha=alpha,\n",
    "            ),\n",
    "            scoring = {\n",
    "                \"acc\": \"accuracy\",\n",
    "                \"recall\": \"recall\",\n",
    "                \"precision\": \"precision\",\n",
    "                \"f1\": \"f1\",\n",
    "                \"roc_auc\": \"roc_auc\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        mean_test_acc = cv_run['test_acc'].mean()\n",
    "        if mean_test_acc > best_parameters[\"test_acc\"]:\n",
    "            best_parameters[\"max_depth\"] = max_depth\n",
    "            best_parameters[\"n_estimators\"] = n_estimators\n",
    "            best_parameters[\"reg_lambda\"] = reg_lambda\n",
    "            best_parameters[\"reg_aplha\"] = alpha\n",
    "            best_parameters[\"learning_rate\"] = lr\n",
    "            best_parameters[\"test_acc\"] = mean_test_acc\n",
    "            best_parameters[\"cv_scores\"] = cv_run\n",
    "            \n",
    "    print(f\"Best parameters XGB: {best_parameters}\")\n",
    "\n",
    "    cv_scores_xgbc = best_parameters[\"cv_scores\"]\n",
    "\n",
    "    pd.DataFrame(cv_scores_xgbc).describe(percentiles=[]).loc[[\"mean\", \"std\", \"min\", \"max\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1df4d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Logistic Regression CV: 100%|██████████| 63/63 [00:13<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters LogClass: {'max_iter': 10, 'cs': 1, 'test_acc': np.float64(0.9344827586206896), 'cv_scores': {'fit_time': array([0.01874685, 0.02794361, 0.02845097, 0.01874685, 0.02738857]), 'score_time': array([0.01403737, 0.01403737, 0.01303887, 0.01403737, 0.01303887]), 'test_acc': array([0.9362069 , 0.92241379, 0.9362069 , 0.92586207, 0.95172414]), 'train_acc': array([0.93405172, 0.9375    , 0.93405172, 0.93663793, 0.93017241]), 'test_recall': array([0.92307692, 0.90939597, 0.91275168, 0.9261745 , 0.95637584]), 'train_recall': array([0.9261745 , 0.92958927, 0.92875105, 0.92539816, 0.91785415]), 'test_precision': array([0.95172414, 0.93771626, 0.96113074, 0.92929293, 0.95      ]), 'train_precision': array([0.94439692, 0.94786325, 0.94217687, 0.95008606, 0.94477998]), 'test_f1': array([0.93718166, 0.92333901, 0.9363167 , 0.92773109, 0.95317726]), 'train_f1': array([0.93519695, 0.93863733, 0.93541579, 0.93757962, 0.93112245]), 'test_roc_auc': array([0.91422773, 0.88958899, 0.91702366, 0.88867866, 0.93888334]), 'train_roc_auc': array([0.90903303, 0.91512193, 0.90825177, 0.91541088, 0.90289109])}}\n",
      "      fit_time  score_time  test_acc  train_acc  test_recall  train_recall  \\\n",
      "mean  0.024255    0.013638  0.934483   0.934483     0.925555      0.925553   \n",
      "std   0.005043    0.000547  0.011437   0.002859     0.018584      0.004642   \n",
      "min   0.018747    0.013039  0.922414   0.930172     0.909396      0.917854   \n",
      "max   0.028451    0.014037  0.951724   0.937500     0.956376      0.929589   \n",
      "\n",
      "      test_precision  train_precision   test_f1  train_f1  test_roc_auc  \\\n",
      "mean        0.945973         0.945861  0.935549  0.935590      0.909680   \n",
      "std         0.012504         0.003112  0.011448  0.002889      0.021048   \n",
      "min         0.929293         0.942177  0.923339  0.931122      0.888679   \n",
      "max         0.961131         0.950086  0.953177  0.938637      0.938883   \n",
      "\n",
      "      train_roc_auc  \n",
      "mean       0.910142  \n",
      "std        0.005243  \n",
      "min        0.902891  \n",
      "max        0.915411  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:18<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 2, 'n_estimators': 50, 'min_samples_leaf': 3, 'test_acc': np.float64(0.9344827586206896), 'cv_scores': {'fit_time': array([0.06159568, 0.0585742 , 0.06008387, 0.06159568, 0.0585742 ]), 'score_time': array([0.01642346, 0.0244987 , 0.01793528, 0.02498651, 0.01893544]), 'test_acc': array([0.9362069 , 0.92241379, 0.9362069 , 0.92586207, 0.95172414]), 'train_acc': array([0.93405172, 0.9375    , 0.93405172, 0.93663793, 0.93017241]), 'test_recall': array([0.92307692, 0.90939597, 0.91275168, 0.9261745 , 0.95637584]), 'train_recall': array([0.9261745 , 0.92958927, 0.92875105, 0.92539816, 0.91785415]), 'test_precision': array([0.95172414, 0.93771626, 0.96113074, 0.92929293, 0.95      ]), 'train_precision': array([0.94439692, 0.94786325, 0.94217687, 0.95008606, 0.94477998]), 'test_f1': array([0.93718166, 0.92333901, 0.9363167 , 0.92773109, 0.95317726]), 'train_f1': array([0.93519695, 0.93863733, 0.93541579, 0.93757962, 0.93112245]), 'test_roc_auc': array([0.9568431 , 0.94492241, 0.96509829, 0.96036818, 0.97051859]), 'train_roc_auc': array([0.96584797, 0.96395195, 0.96191924, 0.96261169, 0.96051167])}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/576 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['score_label_as_int'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 20\u001b[0m\n\u001b[0;32m     10\u001b[0m ft_cols_quantitative \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_spent_Alone\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSocial_event_attendance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPost_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPersonality\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m################\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mrun_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft_cols_qualitative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_cols_qualitative_to_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_cols_quantitative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 122\u001b[0m, in \u001b[0;36mrun_exp\u001b[1;34m(ft_cols_qualitative, ft_cols_qualitative_to_one_hot, ft_cols_quantitative, target_col)\u001b[0m\n\u001b[0;32m    108\u001b[0m best_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m }\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m max_depth, n_estimators, reg_lambda, lr, alpha \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    119\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mproduct(list_max_depth, list_n_estimators, list_lambda, list_learning_rate, list_alpha),\n\u001b[0;32m    120\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(list_max_depth) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(list_n_estimators) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(list_lambda) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(list_learning_rate) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(list_alpha)\n\u001b[0;32m    121\u001b[0m ):\n\u001b[1;32m--> 122\u001b[0m     cv_run \u001b[38;5;241m=\u001b[39m \u001b[43mrun_kfold_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mft_cols_qualitative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_cols_qualitative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mft_cols_qualitative_to_one_hot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_cols_qualitative_to_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mft_cols_quantitative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_cols_quantitative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore_label_as_int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStandardScaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXGBClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreg_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43macc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     mean_test_acc \u001b[38;5;241m=\u001b[39m cv_run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean_test_acc \u001b[38;5;241m>\u001b[39m best_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m, in \u001b[0;36mrun_kfold_pipeline\u001b[1;34m(df_raw, ft_cols_qualitative, ft_cols_qualitative_to_one_hot, ft_cols_quantitative, target_col, scaler, model, n_splits, random_state, shuffle, scoring)\u001b[0m\n\u001b[0;32m     24\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m     25\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     26\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m,    scaler,                                   ft_cols_quantitative),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     34\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[0;32m     35\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[0;32m     36\u001b[0m ])\n\u001b[1;32m---> 38\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m df_raw[target_col]\n\u001b[0;32m     41\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39mn_splits, random_state\u001b[38;5;241m=\u001b[39mrandom_state, shuffle\u001b[38;5;241m=\u001b[39mshuffle)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['score_label_as_int'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "ft_cols_qualitative = [\n",
    "    'Stage_fear',\n",
    "    'Drained_after_socializing',\n",
    "]\n",
    "\n",
    "ft_cols_qualitative_to_one_hot = [\n",
    "    \n",
    "]\n",
    "\n",
    "ft_cols_quantitative = [\n",
    "    'Time_spent_Alone',\n",
    "    'Social_event_attendance',\n",
    "    'Going_outside',\n",
    "    'Friends_circle_size',\n",
    "    'Post_frequency'\n",
    "]\n",
    "\n",
    "target_col = 'Personality' ################\n",
    "\n",
    "run_exp(ft_cols_qualitative, ft_cols_qualitative_to_one_hot, ft_cols_quantitative, target_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
